{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd283622",
   "metadata": {},
   "source": [
    "\n",
    "# SmokeTest — Environment & Mini-Run Validator\n",
    "\n",
    "Use this notebook to quickly verify:\n",
    "1. API keys & model IDs are set and clients initialize.\n",
    "2. `trials_participants.csv` exists and has basic integrity.\n",
    "3. Each provider (OpenAI & Gemini) can return a clean **1–10** on a tiny image.\n",
    "4. A **mini end-to-end run** over a small number of trials (configurable) succeeds.\n",
    "\n",
    "> Run this **before** a full experiment to save time and money.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c1b608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Quick knobs ====\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# How many TrialIDs to run in the mini live run (Phase 1 + Phase 2)?\n",
    "N_TRIALS = 5\n",
    "\n",
    "# Set to True to simulate responses without hitting APIs\n",
    "DRY_RUN = False\n",
    "\n",
    "# Model IDs (override via env if needed)\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "GEMINI_MODEL = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-flash\")\n",
    "\n",
    "# Paths\n",
    "TRIALS_PARTICIPANTS_PATH = Path(\"trials_participants.csv\")\n",
    "LLMRUNNER2_NOTEBOOK = Path(\"LLMRunner2.ipynb\")\n",
    "\n",
    "# Propagate DRY_RUN to runner\n",
    "os.environ[\"DRY_RUN\"] = \"1\" if DRY_RUN else \"0\"\n",
    "\n",
    "print(f\"[cfg] N_TRIALS={N_TRIALS} DRY_RUN={DRY_RUN}\")\n",
    "print(f\"[cfg] OPENAI_MODEL={OPENAI_MODEL}  GEMINI_MODEL={GEMINI_MODEL}\")\n",
    "print(f\"[cfg] trials_participants={TRIALS_PARTICIPANTS_PATH}  runner_nb={LLMRUNNER2_NOTEBOOK}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Environment & client sanity ====\n",
    "import sys\n",
    "\n",
    "if not DRY_RUN:\n",
    "    assert os.getenv(\"OPENAI_API_KEY\"), \"Missing OPENAI_API_KEY\"\n",
    "    assert os.getenv(\"GEMINI_API_KEY\"), \"Missing GEMINI_API_KEY\"\n",
    "\n",
    "# Imports\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"OpenAI SDK missing. Try: pip install openai>=1.30\") from e\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Google GenAI SDK missing. Try: pip install google-generativeai\") from e\n",
    "\n",
    "print(\"[ok] SDK imports succeeded\")\n",
    "\n",
    "# Initialize (no requests yet)\n",
    "if not DRY_RUN:\n",
    "    _oai = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "    _gem = genai.GenerativeModel(GEMINI_MODEL, generation_config={\"temperature\": 0})\n",
    "    print(\"[ok] Clients constructed\")\n",
    "else:\n",
    "    print(\"[info] DRY_RUN=1 — skipping client construction checks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Schedule integrity (fast) ====\n",
    "import pandas as pd\n",
    "\n",
    "assert TRIALS_PARTICIPANTS_PATH.exists(), \"trials_participants.csv not found. Run ImageGen2 export first.\"\n",
    "df = pd.read_csv(TRIALS_PARTICIPANTS_PATH)\n",
    "\n",
    "need = {\"TrialID\",\"Block\",\"ParticipantID\",\"Truth\",\"AssignedCondition\",\"Image\"}\n",
    "assert need.issubset(df.columns), f\"Missing required columns. Need {need}, got {set(df.columns)}\"\n",
    "\n",
    "triplets = df.groupby(\"TrialID\").size()\n",
    "assert (triplets == 3).all(), \"Each TrialID must have exactly 3 participant rows\"\n",
    "\n",
    "# Basic mixture-present composition check on a small sample\n",
    "mix_pre = df[(df[\"Block\"]==\"mixture\") & (df[\"Truth\"]==1)]\n",
    "if not mix_pre.empty:\n",
    "    ct = (mix_pre.groupby([\"TrialID\",\"AssignedCondition\"]).size()\n",
    "          .unstack(fill_value=0))\n",
    "    sample = ct.head(10)\n",
    "    display(sample)\n",
    "    assert ((ct.get(\"strong\",0)==1) & (ct.get(\"weak\",0)==2)).all(), \"Mixture-present must be 1 strong + 2 weak\"\n",
    "else:\n",
    "    print(\"[warn] No mixture-present rows found (small dataset?)\")\n",
    "\n",
    "print(\"[ok] trials_participants.csv basic integrity looks good\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0397564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Path existence check (sample 30 rows) ====\n",
    "from pathlib import Path\n",
    "\n",
    "sample = df.sample(min(30, len(df)), random_state=0)\n",
    "missing = []\n",
    "for _, r in sample.iterrows():\n",
    "    p = Path(str(r[\"Image\"]))\n",
    "    if not p.is_absolute():\n",
    "        p = Path.cwd() / p\n",
    "    if not p.exists():\n",
    "        missing.append(str(p))\n",
    "if missing:\n",
    "    print(\"Missing files (first 10):\", missing[:10])\n",
    "    raise FileNotFoundError(f\"{len(missing)} missing scheduled image files\")\n",
    "print(\"[ok] Sampled image paths exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Single-call provider smoke tests (tiny image) ====\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def parse_confidence(text: str) -> int:\n",
    "    s = (text or \"\").strip()\n",
    "    num = \"\"\n",
    "    for ch in s:\n",
    "        if ch.isdigit():\n",
    "            num += ch\n",
    "        elif num:\n",
    "            break\n",
    "    try:\n",
    "        val = int(num)\n",
    "        return val if 1 <= val <= 10 else -1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "# Build tiny image\n",
    "tmp = Path(\"smoke_present.png\")\n",
    "arr = np.full((64,64), 128, np.uint8)\n",
    "yy, xx = np.ogrid[:64, :64]\n",
    "mask = (yy-32)**2 + (xx-32)**2 <= 7**2\n",
    "arr[mask] = 160\n",
    "Image.fromarray(arr).save(tmp)\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a perceptual decision-maker. Return ONLY one integer 1–10.\"\n",
    "PHASE1_INSTR = \"Is a faint luminance signal present? 1–5=absent, 6–10=present. Return ONLY the integer.\"\n",
    "\n",
    "if not DRY_RUN:\n",
    "    # OpenAI (data URL)\n",
    "    import base64, time\n",
    "    b64 = base64.b64encode(tmp.read_bytes()).decode(\"utf-8\")\n",
    "    client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    t0 = time.time()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[\n",
    "            {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "            {\"role\":\"user\",\"content\":[\n",
    "                {\"type\":\"text\",\"text\":PHASE1_INSTR},\n",
    "                {\"type\":\"image_url\",\"image_url\":{\"url\": f\"data:image/png;base64,{b64}\"}}\n",
    "            ]}\n",
    "        ],\n",
    "        temperature=0, max_tokens=6,\n",
    "    )\n",
    "    oai_text = (resp.choices[0].message.content or \"\").strip()\n",
    "    oai_conf = parse_confidence(oai_text)\n",
    "    print(f\"[OpenAI] raw='{oai_text}' parsed={oai_conf}\")\n",
    "\n",
    "    # Gemini (inline PIL)\n",
    "    import google.generativeai as genai, time\n",
    "    genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL, generation_config={\"temperature\": 0})\n",
    "    img = Image.open(tmp).convert(\"RGB\")\n",
    "    t0 = time.time()\n",
    "    resp = model.generate_content([SYSTEM_PROMPT + \"\\n\\n\" + PHASE1_INSTR, img])\n",
    "    g_text = (resp.text or \"\").strip()\n",
    "    g_conf = parse_confidence(g_text)\n",
    "    print(f\"[Gemini] raw='{g_text}' parsed={g_conf}\")\n",
    "\n",
    "    assert 1 <= oai_conf <= 10, \"OpenAI smoke test did not return 1–10\"\n",
    "    assert 1 <= g_conf <= 10, \"Gemini smoke test did not return 1–10\"\n",
    "    print(\"[ok] Both providers returned clean 1–10 integers\")\n",
    "else:\n",
    "    print(\"[info] DRY_RUN=1 — skipping provider smoke tests\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f036999b",
   "metadata": {},
   "source": [
    "\n",
    "## Mini End-to-End Run (Phase 1 + Phase 2)\n",
    "\n",
    "Executes the participant-scheduled run using **LLMRunner2** over `N_TRIALS` TrialIDs.\n",
    "- Honors `DRY_RUN` (set at the top).\n",
    "- Writes results into `API_files/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9985c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nbformat\n",
    "\n",
    "assert LLMRUNNER2_NOTEBOOK.exists(), \"LLMRunner2.ipynb not found. Generate it first.\"\n",
    "\n",
    "# Execute code cells from LLMRunner2 in this kernel\n",
    "nb_runner = nbformat.read(LLMRUNNER2_NOTEBOOK.open(\"r\", encoding=\"utf-8\"), as_version=4)\n",
    "for cell in nb_runner.cells:\n",
    "    if cell.cell_type == \"code\" and cell.source.strip():\n",
    "        exec(compile(cell.source, \"<LLMRunner2_cell>\", \"exec\"), globals())\n",
    "\n",
    "# Now run a limited set\n",
    "from pathlib import Path as _Path\n",
    "dfp = load_trials_participants(_Path(TRIALS_PARTICIPANTS_PATH))\n",
    "p1 = run_phase1(dfp, limit_trials=int(N_TRIALS))\n",
    "p2 = run_phase2(p1)\n",
    "\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "p1.to_csv(PHASE1_CSV, index=False)\n",
    "p2.to_csv(PHASE2_CSV, index=False)\n",
    "consensus = consensus_from_phase2(p2)\n",
    "consensus.to_csv(CONSENSUS_CSV, index=False)\n",
    "\n",
    "print(f\"[ok] Wrote: {PHASE1_CSV}, {PHASE2_CSV}, {CONSENSUS_CSV}\")\n",
    "display(p1.head(3)); display(p2.head(3)); display(consensus.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: quick summaries (works in DRY_RUN too)\n",
    "summarize_phase(p1, \"Phase 1 — Independent (mini)\")\n",
    "summarize_phase(p2, \"Phase 2 — Discussion (mini)\")\n",
    "summarize_delta(p1, p2)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
