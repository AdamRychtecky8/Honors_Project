{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784580dc",
   "metadata": {},
   "source": [
    "\n",
    "# Perceptual Task Replication — LLMs as Participants\n",
    "\n",
    "This notebook replicates the **perceptual detection task** from Juni & Eckstein (2015), but replaces human participants with **LLMs**.\n",
    "\n",
    "On each trial:\n",
    "\n",
    "- An LLM \"participant\" views a noisy image and reports a **confidence rating** on a 1–10 scale:  \n",
    "  - **1–5** → Signal **ABSENT** (1 = highest confidence absent)  \n",
    "  - **6–10** → Signal **PRESENT** (10 = highest confidence present)  \n",
    "- These responses mimic the paper’s 10-point confidence scale.  \n",
    "- Results are collected for multiple providers (e.g., **OpenAI** and **Gemini**).\n",
    "\n",
    "---\n",
    "\n",
    "### Conditions\n",
    "- **Equal** → All participants see comparable signal strength. Majority rules tend to perform near-optimally.  \n",
    "- **Mixture** → One participant sees a strong signal, two see weak signals. Majority can be suboptimal; alternative group rules may explain behavior better.\n",
    "\n",
    "---\n",
    "\n",
    "### Inputs\n",
    "- Stimulus images (PNG files) generated in `allimgs/`.  \n",
    "- Metadata file `trials.csv` containing:  \n",
    "  - `TrialID`, `Condition` (`equal` or `mixture`), `Truth` (0/1),  \n",
    "  - `Image` (path to the stimulus).  \n",
    "\n",
    "---\n",
    "\n",
    "### Outputs\n",
    "- A CSV (`llm_results.csv`) containing per-trial responses with confidence, binary predictions, correctness, and latency.  \n",
    "- Diagnostic plots and summary tables:  \n",
    "  - Accuracy overall and by condition  \n",
    "  - Mean confidence for absent vs present trials  \n",
    "  - Example stimulus previews for quality control  \n",
    "\n",
    "---\n",
    "\n",
    "### API Keys\n",
    "Environment variables (never hard-coded):\n",
    "- `OPENAI_API_KEY`  \n",
    "- `GEMINI_API_KEY`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3add6bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install --quiet google-generativeai openai pillow pandas numpy tqdm matplotlib\n",
    "\n",
    "import os, base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# APIs\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Where the generator put things\n",
    "TRIALS_CANDIDATES = [\"trials.csv\", os.path.join(\"allimgs\", \"index.csv\")]\n",
    "RESULTS_CSV = \"llm_results.csv\"\n",
    "\n",
    "# Models (override via env if you want)\n",
    "OPENAI_MODEL  = os.getenv(\"OPENAI_MODEL\",  \"gpt-4o-mini\")\n",
    "GEMINI_MODEL  = os.getenv(\"GEMINI_MODEL\",  \"gemini-2.0-flash-thinking-exp-01-21\")\n",
    "\n",
    "# Expect env keys in Codespaces/terminal\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "print(f\"[cfg] trials={TRIALS_CANDIDATES}\")\n",
    "print(f\"[cfg] providers=['OpenAI','Gemini'] models={[OPENAI_MODEL, GEMINI_MODEL]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62884484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Sequence\n",
    "\n",
    "def _first_existing(paths: Sequence[str]) -> str:\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not find trials.csv. Run your image generator notebook first.\")\n",
    "\n",
    "def load_trials() -> pd.DataFrame:\n",
    "    path = _first_existing(TRIALS_CANDIDATES)\n",
    "    df = pd.read_csv(path) if path.endswith(\".csv\") else pd.read_excel(path)\n",
    "\n",
    "    need = {\"TrialID\",\"Condition\",\"Truth\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise ValueError(f\"Trials file missing required columns {need}. Found {list(df.columns)}\")\n",
    "\n",
    "    if \"Image\" not in df.columns:\n",
    "        raise ValueError(\"Expected an 'Image' column with PNG paths (from allimgs/).\")\n",
    "\n",
    "    df[\"Condition\"] = df[\"Condition\"].astype(str).str.strip().str.lower()\n",
    "    df[\"Truth\"] = df[\"Truth\"].astype(int)\n",
    "    return df\n",
    "\n",
    "df_trials = load_trials()\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "df_trials[\"Image\"] = df_trials[\"Image\"].apply(lambda p: str(PROJECT_ROOT / p))\n",
    "print(f\"[data] {len(df_trials)} trials\")\n",
    "display(df_trials.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b31069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert OPENAI_API_KEY, \"Missing OPENAI_API_KEY env var.\"\n",
    "assert GEMINI_API_KEY, \"Missing GEMINI_API_KEY env var.\"\n",
    "\n",
    "# Clients\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "gemini = genai.GenerativeModel(GEMINI_MODEL)\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are shown a noisy grayscale image that may or may not contain a faint Gaussian luminance blob.\\n\"\n",
    "    \"Your task is to decide if the blob is present or absent AND indicate your confidence.\\n\\n\"\n",
    "    \"Respond with ONLY a single integer from 1 to 10, with no other text:\\n\"\n",
    "    \"- 1 = ABSENT, highest confidence\\n\"\n",
    "    \"- 2–5 = ABSENT, decreasing confidence\\n\"\n",
    "    \"- 6–9 = PRESENT, increasing confidence\\n\"\n",
    "    \"- 10 = PRESENT, highest confidence\"\n",
    ")\n",
    "\n",
    "def encode_image_b64(path: str) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "def parse_confidence(text: str) -> int:\n",
    "    try:\n",
    "        val = int(\"\".join(ch for ch in text if ch.isdigit()))\n",
    "        if 1 <= val <= 10:\n",
    "            return val\n",
    "    except:\n",
    "        pass\n",
    "    return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8902ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_openai(image_path: str, model: str = OPENAI_MODEL) -> dict:\n",
    "    img_b64 = encode_image_b64(image_path)\n",
    "    resp = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": (\n",
    "                        \"Decide if a faint Gaussian luminance blob is present in this noisy image.\\n\"\n",
    "                        \"Respond with ONLY a single integer from 1 to 10:\\n\"\n",
    "                        \"- 1 = ABSENT with highest confidence\\n\"\n",
    "                        \"- 2–5 = ABSENT with decreasing confidence\\n\"\n",
    "                        \"- 6–9 = PRESENT with increasing confidence\\n\"\n",
    "                        \"- 10 = PRESENT with highest confidence\"\n",
    "                    )\n",
    "                },\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{img_b64}\"}},\n",
    "            ]}\n",
    "        ],\n",
    "        max_tokens=5,\n",
    "        temperature=0\n",
    "    )\n",
    "    out = resp.choices[0].message.content or \"\"\n",
    "    return {\"raw\": out, \"decision\": parse_confidence(out)}\n",
    "\n",
    "def call_gemini(image_path: str, model_obj=gemini) -> dict:\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    prompt = (\n",
    "        SYSTEM_PROMPT + \"\\n\\n\"\n",
    "        \"Decide if a faint Gaussian luminance blob is present in this noisy image.\\n\"\n",
    "        \"Respond with ONLY a single integer from 1 to 10:\\n\"\n",
    "        \"- 1 = ABSENT with highest confidence\\n\"\n",
    "        \"- 2–5 = ABSENT with decreasing confidence\\n\"\n",
    "        \"- 6–9 = PRESENT with increasing confidence\\n\"\n",
    "        \"- 10 = PRESENT with highest confidence\"\n",
    "    )\n",
    "    resp = model_obj.generate_content([prompt, img])\n",
    "    out = (resp.text or \"\").strip()\n",
    "    return {\"raw\": out, \"decision\": parse_confidence(out)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41dceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quick diagnostic plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "df_trials[\"Condition\"].value_counts().sort_index().plot(kind=\"bar\", ax=axes[0], title=\"Trials per condition\")\n",
    "(df_trials[\"Truth\"].map({0: \"absent\", 1: \"present\"})\n",
    " .value_counts()\n",
    " .reindex([\"absent\", \"present\"])\n",
    " .fillna(0)\n",
    ").plot(kind=\"bar\", ax=axes[1], title=\"Label balance\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Peek one absent and one present\n",
    "absent_row  = df_trials[df_trials[\"Truth\"] == 0].head(1)\n",
    "present_row = df_trials[df_trials[\"Truth\"] == 1].head(1)\n",
    "sample_rows = pd.concat([absent_row, present_row])\n",
    "sample_paths = sample_rows[\"Image\"].astype(str).tolist()\n",
    "labels = sample_rows[\"Truth\"].map({0: \"absent\", 1: \"present\"}).tolist()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(sample_paths), figsize=(6,3))\n",
    "if len(sample_paths) == 1:\n",
    "    axes = [axes]\n",
    "for ax, path, lab in zip(axes, sample_paths, labels):\n",
    "    if Path(path).exists():\n",
    "        im = Image.open(path).convert(\"L\")\n",
    "        ax.imshow(im, cmap=\"gray\", vmin=0, vmax=255)\n",
    "        ax.set_title(f\"{lab}\\n{Path(path).name}\", fontsize=9)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"Missing file\", ha=\"center\", va=\"center\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc8e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from time import time\n",
    "\n",
    "def run_llms(df: pd.DataFrame, limit: int | None = 50) -> pd.DataFrame:\n",
    "    it = df if limit is None else df.head(limit)\n",
    "    rows = []\n",
    "\n",
    "    for _, r in tqdm(it.iterrows(), total=len(it), desc=\"LLM trials\"):\n",
    "        trial_id  = int(r[\"TrialID\"])\n",
    "        cond      = str(r[\"Condition\"]).strip().lower()\n",
    "        truth     = int(r[\"Truth\"])\n",
    "        img_path  = str(r[\"Image\"])\n",
    "\n",
    "        if not Path(img_path).exists():\n",
    "            rows.append({\n",
    "                \"TrialID\": trial_id, \"Condition\": cond, \"Truth\": truth, \"Image\": img_path,\n",
    "                \"provider\": \"OpenAI\", \"model\": OPENAI_MODEL,\n",
    "                \"raw\": \"ERROR: file not found\", \"decision\": -1, \"latency_s\": -1, \"correct\": 0\n",
    "            })\n",
    "            rows.append({\n",
    "                \"TrialID\": trial_id, \"Condition\": cond, \"Truth\": truth, \"Image\": img_path,\n",
    "                \"provider\": \"Gemini\", \"model\": GEMINI_MODEL,\n",
    "                \"raw\": \"ERROR: file not found\", \"decision\": -1, \"latency_s\": -1, \"correct\": 0\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # --- OpenAI ---\n",
    "        try:\n",
    "            t0 = time()\n",
    "            oai = call_openai(img_path)\n",
    "            t1 = time()\n",
    "            rows.append({\n",
    "                \"TrialID\": trial_id, \"Condition\": cond, \"Truth\": truth, \"Image\": img_path,\n",
    "                \"provider\": \"OpenAI\", \"model\": OPENAI_MODEL,\n",
    "                \"raw\": oai.get(\"raw\",\"\"), \"decision\": int(oai.get(\"decision\",-1)),\n",
    "                \"latency_s\": round(t1 - t0, 3),\n",
    "                \"correct\": int(int(oai.get(\"decision\",-1)) >= 6) == truth if oai.get(\"decision\",-1) in range(1,11) else 0,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            rows.append({\n",
    "                \"TrialID\": trial_id, \"Condition\": cond, \"Truth\": truth, \"Image\": img_path,\n",
    "                \"provider\": \"OpenAI\", \"model\": OPENAI_MODEL,\n",
    "                \"raw\": f\"ERROR: {e}\", \"decision\": -1, \"latency_s\": -1, \"correct\": 0\n",
    "            })\n",
    "\n",
    "        # --- Gemini ---\n",
    "        try:\n",
    "            t1 = time()\n",
    "            gmi = call_gemini(img_path)\n",
    "            t2 = time()\n",
    "            rows.append({\n",
    "                \"TrialID\": trial_id, \"Condition\": cond, \"Truth\": truth, \"Image\": img_path,\n",
    "                \"provider\": \"Gemini\", \"model\": GEMINI_MODEL,\n",
    "                \"raw\": gmi.get(\"raw\",\"\"), \"decision\": int(gmi.get(\"decision\",-1)),\n",
    "                \"latency_s\": round(t2 - t1, 3),\n",
    "                \"correct\": int(int(gmi.get(\"decision\",-1)) >= 6) == truth if gmi.get(\"decision\",-1) in range(1,11) else 0,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            rows.append({\n",
    "                \"TrialID\": trial_id, \"Condition\": cond, \"Truth\": truth, \"Image\": img_path,\n",
    "                \"provider\": \"Gemini\", \"model\": GEMINI_MODEL,\n",
    "                \"raw\": f\"ERROR: {e}\", \"decision\": -1, \"latency_s\": -1, \"correct\": 0\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f93ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize(df: pd.DataFrame):\n",
    "    if df is None or df.empty:\n",
    "        print(\"[warn] no results\"); \n",
    "        return\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"decision\"] = pd.to_numeric(df[\"decision\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df = df[df[\"decision\"].between(1, 10, inclusive=\"both\")]\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"[warn] no valid 1–10 decisions to summarize.\")\n",
    "        return\n",
    "\n",
    "    df[\"pred\"] = (df[\"decision\"] >= 6).astype(int)\n",
    "    df[\"correct\"] = (df[\"pred\"] == df[\"Truth\"]).astype(int)\n",
    "\n",
    "    overall_acc = df.groupby(\"provider\")[\"correct\"].mean().rename(\"accuracy\").to_frame().reset_index()\n",
    "    by_cond_acc = df.groupby([\"provider\", \"Condition\"])[\"correct\"].mean().rename(\"accuracy\").to_frame().reset_index()\n",
    "    mean_conf = df.groupby([\"provider\", \"Truth\"])[\"decision\"].mean().rename(\"mean_confidence\").to_frame().reset_index().replace({\"Truth\": {0: \"absent\", 1: \"present\"}})\n",
    "\n",
    "    print(\"Overall accuracy\")\n",
    "    display(overall_acc)\n",
    "    print(\"\\nAccuracy by condition\")\n",
    "    display(by_cond_acc)\n",
    "    print(\"\\nMean confidence by ground truth\")\n",
    "    display(mean_conf)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    axes[0].set_title(\"Overall accuracy\")\n",
    "    axes[0].bar(overall_acc[\"provider\"], overall_acc[\"accuracy\"])\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    pv_acc = by_cond_acc.pivot(index=\"Condition\", columns=\"provider\", values=\"accuracy\").fillna(0)\n",
    "    pv_acc.plot(kind=\"bar\", ax=axes[1], title=\"Accuracy by condition\", ylim=(0, 1))\n",
    "    axes[1].set_xlabel(\"Condition\")\n",
    "    axes[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    pv_conf = mean_conf.pivot(index=\"Truth\", columns=\"provider\", values=\"mean_confidence\").reindex([\"absent\",\"present\"])\n",
    "    pv_conf.plot(kind=\"bar\", ax=axes[2], title=\"Mean confidence by truth\", ylim=(1, 10))\n",
    "    axes[2].set_xlabel(\"Ground truth\")\n",
    "    axes[2].set_ylabel(\"Mean confidence (1–10)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
